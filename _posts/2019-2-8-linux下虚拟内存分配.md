---
layout: post
title: "linux下虚拟内存分配"
date: 2019-2-8
excerpt: ""
tags: [linux内核,虚拟内存分配]
comments: true
---

## 描述物理内存

- NUMA模型

![](../assets/img/numa.jpeg)

- - 一台计算机有多个Node，每个节点有多个core，上图中是4个核，在每个节点内存中都有自己的内存，称为本地内存。在本地内存相对的是远端内存，对于node0来说，node1、node2、node3中的内存都是远端内存。在节点中，CPU与内存之间通过片内总线进行连接。各个节点之间通过互联模块进行连接。NUMA节点中CPU对节点内部内存与节点外部内存的访问是有差异的。

- **节点、管理区和页面的关系**

  ![](../assets/img/节点_管理区_页面的关系.png)

- **节点**

  - 内存中的每个节点都由pg_data_t描述，而pg_data_t由struct  pglist_data 定义而来。

    ```c
    //<include/linux/mmzone.h>
    struct bootmem_data;
    typedef struct pglist_data {
    	zone_t node_zones[MAX_NR_ZONES];
    	zonelist_t node_zonelists[GFP_ZONEMASK+1];
    	int nr_zones;
    	struct page *node_mem_map;
    	unsigned long *valid_addr_bitmap;
    	struct bootmem_data *bdata;//内存引导程序
    	unsigned long node_start_paddr;//节点起始物理地址
    	unsigned long node_start_mapnr;//节点在全局mem_map中的页面偏移
    	unsigned long node_size;
    	int node_id;
    	struct pglist_data *node_next;
    } pg_data_t;
    ```

- **管理区**

  - 每个管理区由struct zone_t 描述。zone_structs用于跟踪诸如页面使用情况统计数，空闲区域信息和锁信息。

    ```c
    typedef struct zone_struct {
    	/*
    	 * Commonly accessed fields:
    	 */
    	spinlock_t		lock;
    	unsigned long		free_pages;
    	unsigned long		pages_min, pages_low, pages_high;
    	int			need_balance;
    
    	/*
    	 * free areas of different sizes
    	 */
    	free_area_t		free_area[MAX_ORDER];
    
    	/*
    	 * wait_table		-- the array holding the hash table
    	 * wait_table_size	-- the size of the hash table array
    	 * wait_table_shift	-- wait_table_size
    	 * 				== BITS_PER_LONG (1 << wait_table_bits)
    	 *
    	 * The purpose of all these is to keep track of the people
    	 * waiting for a page to become available and make them
    	 * runnable again when possible. The trouble is that this
    	 * consumes a lot of space, especially when so few things
    	 * wait on pages at a given time. So instead of using
    	 * per-page waitqueues, we use a waitqueue hash table.
    	 *
    	 * The bucket discipline is to sleep on the same queue when
    	 * colliding and wake all in that wait queue when removing.
    	 * When something wakes, it must check to be sure its page is
    	 * truly available, a la thundering herd. The cost of a
    	 * collision is great, but given the expected load of the
    	 * table, they should be so rare as to be outweighed by the
    	 * benefits from the saved space.
    	 *
    	 * __wait_on_page() and unlock_page() in mm/filemap.c, are the
    	 * primary users of these fields, and in mm/page_alloc.c
    	 * free_area_init_core() performs the initialization of them.
    	 */
    	wait_queue_head_t	* wait_table;
    	unsigned long		wait_table_size;
    	unsigned long		wait_table_shift;
    
    	/*
    	 * Discontig memory support fields.
    	 */
    	struct pglist_data	*zone_pgdat;
    	struct page		*zone_mem_map;
    	unsigned long		zone_start_paddr;
    	unsigned long		zone_start_mapnr;
    
    	/*
    	 * rarely used fields:
    	 */
    	char			*name;
    	unsigned long		size;
    } zone_t;
    
    ```

    free_pages：该管理区中空闲页的总数

    pages_min,pages_low,pages_high:这些都是管理区极值

    need_balance：该标志位通知页面换出kswapd平衡该管理区。当可用页面的数量达到管理区极值的某一个值时，就需要平衡该管理区了

    free_area：空闲区域位图，由伙伴系统使用

- **管理区等待列表**

  - 当页面需要进行I/O操作时，比如页面换入换出,I/O必须被锁住以防止访问到不一致的数据。使用这些页面的进程必须在I/O能访问以前，通过调用wait_on_page()被添加到一个等待队列中。当I/O 完成后，页面通过UnlockPage()解锁，然后等待队列上的每个进程都被唤醒。linux将等待队列的哈希表存储在wait->table中。在发生哈希冲突时，虽然进程也有可能会被唤醒，但不会发生如此频繁。

- mem_map

  - mem_map区域在NUMA系统中，全局mem_map被处理为一个起始于PAGE_OFFSET的虚拟数组。free_area_init_node()函数在系统中被每一个活动节点所调用。

    ```c
    void __init free_area_init_node(int nid, pg_data_t *pgdat, struct page *pmap,
    	unsigned long *zones_size, unsigned long zone_start_paddr, 
    	unsigned long *zholes_size)
    {
    	int i, size = 0;
    	struct page *discard;
    
    	if (mem_map == (mem_map_t *)NULL)
    		mem_map = (mem_map_t *)PAGE_OFFSET;
    
    	free_area_init_core(nid, pgdat, &discard, zones_size, zone_start_paddr,zholes_size, pmap);
    	pgdat->node_id = nid;
    
    	/*
    	 * Get space for the valid bitmap.
    	 */
    	for (i = 0; i < MAX_NR_ZONES; i++)
    		size += zones_size[i];
    	size = LONG_ALIGN((size + 7) >> 3);
    	pgdat->valid_addr_bitmap = (unsigned long *)alloc_bootmem_node(pgdat, size);
    	memset(pgdat->valid_addr_bitmap, 0, size);
    }
    ```

  - NUMA中，分配给lmem_map的内存在他们自己的内存节点中。全局mem_map起始于PAGE_OFFSET的虚拟数组。局部映射的地址存储在pg_data_t->node_mem_map中，也存在于虚拟mem_map中。

    ```c
    void __init free_area_init_core(int nid, pg_data_t *pgdat, struct page **gmap,
    	unsigned long *zones_size, unsigned long zone_start_paddr, 
    	unsigned long *zholes_size, struct page *lmem_map)
    {
        //...
        lmem_map = (struct page *) alloc_bootmem_node(pgdat, map_size);
    }
    ```

- 页面

  - ```c
    typedef struct page {
    	struct list_head list;		/* ->mapping has some page lists. */
    	struct address_space *mapping;	/* The inode (or ...) we belong to. */
    	unsigned long index;		/* Our offset within mapping. */
    	struct page *next_hash;		/* Next page sharing our hash bucket in
    					   the pagecache hash table. */
    	atomic_t count;			/* Usage count, see below. */
    	unsigned long flags;		/* atomic flags, some possibly
    					   updated asynchronously */
    	struct list_head lru;		/* Pageout list, eg. active_list;
    					   protected by pagemap_lru_lock !! */
    	struct page **pprev_hash;	/* Complement to *next_hash. */
    	struct buffer_head * buffers;	/* Buffer maps us to a disk block. */
    
    	/*
    	 * On machines where all RAM is mapped into kernel address space,
    	 * we can simply calculate the virtual address. On machines with
    	 * highmem some memory is mapped into kernel virtual memory
    	 * dynamically, so we need a place to store that address.
    	 * Note that this field could be 16 bits on x86 ... ;)
    	 *
    	 * Architectures with slow multiplication can define
    	 * WANT_PAGE_VIRTUAL in asm/page.h
    	 */
    #if defined(CONFIG_HIGHMEM) || defined(WANT_PAGE_VIRTUAL)
    	void *virtual;			/* Kernel virtual address (NULL if
    					   not kmapped, ie. highmem) */
    #endif /* CONFIG_HIGMEM || WANT_PAGE_VIRTUAL */
    } mem_map_t;
    ```

## 页表管理

- 描述页目录

  - 进程中都有一个指向自己的PGD(页目录项 一级页表)，本质是一个物理页帧。该帧包括了一个pgd_t类型的数组，进程页表载入是通过mm_sturct->pgd复制 到cr3寄存器完成。

    - ```c
      struct mm_struct {    //内存描述符  
      	struct vm_area_struct * mmap;		/* list of VMAs */
      	rb_root_t mm_rb;
      	struct vm_area_struct * mmap_cache;	/* last find_vma result */
      	pgd_t * pgd;
      	atomic_t mm_users;			/* How many users with user space? */
      	atomic_t mm_count;			/* How many references to "struct mm_struct" (users count as 1) */
      	int map_count;				/* number of VMAs */
      	struct rw_semaphore mmap_sem;
      	spinlock_t page_table_lock;		/* Protects task page tables and mm->rss */
      
      	struct list_head mmlist;		/* List of all active mm's.  These are globally strung
      						 * together off init_mm.mmlist, and are protected
      						 * by mmlist_lock
      						 */
      
      	unsigned long start_code, end_code, start_data, end_data;
      	unsigned long start_brk, brk, start_stack;
      	unsigned long arg_start, arg_end, env_start, env_end;
      	unsigned long rss, total_vm, locked_vm;
      	unsigned long def_flags;
      	unsigned long cpu_vm_mask;
      	unsigned long swap_address;
      
      	unsigned dumpable:1;
      
      	/* Architecture-specific MM context */
      	mm_context_t context;
      };
      ```

    - ![](../assets/img/页表布局.png)
